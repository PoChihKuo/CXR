{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Test and report metrics",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PoChihKuo/RECA-CXR/blob/master/Test_and_report_metrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bfPByjjh7s5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hwfrAocTs92n",
        "colab": {}
      },
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import datetime\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import multiprocessing\n",
        "from enum import Enum\n",
        "from google.cloud import bigquery\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.metrics\n",
        "import numpy as np\n",
        "import subprocess\n",
        "import re\n",
        "\n",
        "try:\n",
        "  from google.colab import auth\n",
        "  IN_COLAB = True\n",
        "  auth.authenticate_user()\n",
        "except ImportError:\n",
        "  IN_COLAB = False\n",
        "\n",
        "account = subprocess.check_output(\n",
        "    ['gcloud', 'config', 'list', 'account', '--format',\n",
        "     'value(core.account)']).decode().strip()\n",
        "MY_DIRECTORY = re.sub(r'[^\\w]', '_', account)[:128]\n",
        "\n",
        "%config InlineBackend.figure_format = 'svg'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMmWeNk0pofx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.enable_eager_execution()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMbMHhgDvLEy",
        "colab_type": "text"
      },
      "source": [
        "#The model and testing data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Xe6-vX_3ob7a",
        "colab": {}
      },
      "source": [
        "#Model-MMC\n",
        "PRETRAINED_KERAS_MODEL = 'gs://hst-953-2019-shared-files/mimic-cxr-models/bfantasykuo_gmail_com/model_mimic_gen_aug_epoch5.h5' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EIl8YIgcHOR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Photo-MIMIC\n",
        "VALID_TFRECORDS = 'gs://hst-953-2019-shared-files/mimic-cxr-validationdata/bfantasykuo_gmail_com/Smartphone_frontal*'   \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIQNlJRIvpX3",
        "colab_type": "text"
      },
      "source": [
        "#Define the label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4R8Amh9EyNvG",
        "colab": {}
      },
      "source": [
        "class Labels(Enum):\n",
        "  no_finding = 0\n",
        "  enlarged_cardiomediastinum = 1\n",
        "  cardiomegaly = 2\n",
        "  airspace_opacity = 3\n",
        "  lung_lesion = 4\n",
        "  edema = 5\n",
        "  consolidation = 6\n",
        "  pneumonia = 7\n",
        "  atelectasis = 8\n",
        "  pneumothorax = 9\n",
        "  pleural_effusion = 10\n",
        "  pleural_other = 11\n",
        "  fracture = 12\n",
        "  support_devices = 13\n",
        "\n",
        "\n",
        "class LabelValues(Enum):\n",
        "  not_mentioned = 0\n",
        "  negative = 1\n",
        "  uncertain = 2\n",
        "  positive = 3\n",
        "\n",
        "\n",
        "class Views(Enum):\n",
        "  frontal = 0\n",
        "  lateral = 1\n",
        "  other = 2\n",
        "\n",
        "\n",
        "class Datasets(Enum):\n",
        "  train = 0\n",
        "  valid = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiJfl7H7v5Eg",
        "colab_type": "text"
      },
      "source": [
        "#Define functions and parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "G8M4abgmb8X7",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 1  \n",
        "NUM_EPOCHS = 2  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4PGxc15Bb8E6",
        "colab": {}
      },
      "source": [
        "# label -> probability table: 0 -> 0, 1 -> 0, 2 -> u, 3 -> 1\n",
        "probabs_lookup = tf.constant([0.0, 0.0, 0.0, 1.0])\n",
        "# label -> weight table: 0 -> 1, 1 -> 1, 2 -> w, 3 -> 1\n",
        "weights_lookup = tf.constant([1.0, 1.0, 1.0, 1.0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q_4UP-Wwyx3t",
        "colab": {}
      },
      "source": [
        "feature_description = {'jpg_bytes': tf.io.FixedLenFeature([], tf.string),'patient': tf.io.FixedLenFeature([], tf.int64),'study': tf.io.FixedLenFeature([], tf.int64),'image': tf.io.FixedLenFeature([], tf.int64)}\n",
        "for l in Labels:\n",
        "  feature_description[l.name] = tf.io.FixedLenFeature([], tf.int64)\n",
        "# The height, width, and number of channels of the input images\n",
        "INPUT_HWC = (320, 320, 1)\n",
        "\n",
        "def parse_function(example):\n",
        "  \"\"\"Convert a TFExample from a TFRecord into an input and its true label.\n",
        "\n",
        "    Args:\n",
        "      example (tf.train.Example): A training example read from a TFRecord.\n",
        "\n",
        "    Returns:\n",
        "      Tuple[tf.Tensor, tf.Tensor]: The X-ray image and its labels. The labels\n",
        "        are represented as two stacked arrays. One array is the probability\n",
        "        that this label exists in the image, the other is how much weight this\n",
        "        label should have when training the model.\n",
        "    \"\"\"\n",
        "  parsed = tf.io.parse_single_example(example, feature_description)\n",
        "  # Turn the JPEG data into a matrix of pixel intensities\n",
        "  image = tf.io.decode_jpeg(parsed['jpg_bytes'], channels=1)\n",
        "  # Give the image a definite size, which is needed by TPUs\n",
        "  image = tf.reshape(image, INPUT_HWC)\n",
        "  # Normalize the pixel values to be between 0 and 1\n",
        "  scaled_image = (1.0 / 255.0) * tf.cast(image, tf.float32)\n",
        "  # Combine the labels into an array\n",
        "  labels = tf.stack([parsed[l.name] for l in Labels], axis=0)\n",
        "  # Convert the labels into probabilities and weights using lookup tables.\n",
        "  \n",
        "\n",
        "  probs = tf.gather(probabs_lookup, labels)\n",
        "  weights = tf.gather(weights_lookup, labels)\n",
        "  # Return the input to the model and the true labels\n",
        "  return scaled_image, tf.stack([probs, weights], axis=0)\n",
        "\n",
        "def parse_image_name(example):\n",
        "\n",
        "  parsed = tf.io.parse_single_example(example, feature_description)\n",
        "\n",
        "  subject_num = tf.stack(parsed['patient'], axis=0)\n",
        "  study_num = tf.stack(parsed['study'], axis=0)\n",
        "  image_num = tf.stack(parsed['image'], axis=0)\n",
        "\n",
        "  return subject_num, study_num, image_num\n",
        "\n",
        "def parse_original_label(example):\n",
        "\n",
        "  parsed = tf.io.parse_single_example(example, feature_description)\n",
        "  labels = tf.stack([parsed[l.name] for l in Labels], axis=0)\n",
        "\n",
        "  return labels\n",
        "\n",
        "def get_dataset(valid=False):\n",
        "  \"\"\"Construct a pipeline for loading the data.\n",
        "\n",
        "    Args:\n",
        "      valid (bool): If this is True, use the validation dataset instead of the\n",
        "        training dataset.\n",
        "\n",
        "    Returns:\n",
        "      tf.data.Dataset: A dataset loading pipeline ready for training.\n",
        "    \"\"\"\n",
        "  n_cpu = multiprocessing.cpu_count()\n",
        "  tf_records = VALID_TFRECORDS if valid else TRAIN_TFRECORDS\n",
        "  dataset = tf.data.TFRecordDataset(\n",
        "      tf.io.gfile.glob(tf_records),\n",
        "      buffer_size=16 * 1024 * 1024,\n",
        "      num_parallel_reads=n_cpu)\n",
        "  if not valid:\n",
        "    dataset = dataset.shuffle(256)\n",
        "    dataset = dataset.repeat()\n",
        "  dataset = dataset.map(parse_function, num_parallel_calls=n_cpu)\n",
        "  dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "  dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "798l9Ht1UAOd",
        "colab": {}
      },
      "source": [
        "#@title Accelerators {run: \"auto\"}\n",
        "ACCELERATOR_TYPE = 'Multi-GPU'  #@param [\"Single/Multi-TPU\", \"Single-GPU\", \"Multi-GPU\", \"CPU\"] {type: \"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iKFe1cWM7FB8",
        "colab": {}
      },
      "source": [
        "if ACCELERATOR_TYPE == 'Single/Multi-TPU':\n",
        "  if IN_COLAB:\n",
        "    tpu_name = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "  else:\n",
        "    tpu_name = os.environ['TPU_NAME']\n",
        "  resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu=tpu_name)\n",
        "  tf.contrib.distribute.initialize_tpu_system(resolver)\n",
        "  strategy = tf.contrib.distribute.TPUStrategy(resolver, steps_per_run=100)\n",
        "elif ACCELERATOR_TYPE == 'Multi-GPU':\n",
        "  strategy = tf.distribute.MirroredStrategy()\n",
        "else:\n",
        "  strategy = tf.distribute.get_strategy()  # Default strategy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0D_sio9W7Txy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weighted_binary_crossentropy(prob_weight_y_true, y_pred):\n",
        "  \"\"\"Binary cross-entropy loss function with per-sample weights.\"\"\"\n",
        "  prob_weight_y_true = tf.reshape(prob_weight_y_true, (-1, 2, len(Labels)))\n",
        "  # Unpack the second output of our data pipeline into true probabilities and\n",
        "  # weights for each label.\n",
        "  probs = prob_weight_y_true[:, 0]\n",
        "  weights = prob_weight_y_true[:, 1]\n",
        "  return tf.compat.v1.losses.sigmoid_cross_entropy(\n",
        "      probs,\n",
        "      y_pred,\n",
        "      weights,\n",
        "      reduction=tf.compat.v1.losses.Reduction.SUM_OVER_BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ei4Pa16psimM",
        "colab": {}
      },
      "source": [
        "def download_from_gcs(gcs_path, local_path):\n",
        "  with tf.io.gfile.GFile(gcs_path, 'rb') as gcs_file:\n",
        "    with tf.io.gfile.GFile(local_path, 'wb') as local_file:\n",
        "      local_file.write(gcs_file.read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpQpD-63FpOv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_dataset_label(valid=False):    \n",
        "  n_cpu = multiprocessing.cpu_count()\n",
        "  tf_records = VALID_TFRECORDS if valid else TRAIN_TFRECORDS\n",
        "  Data_size = N_VALID if valid else N_TRAIN\n",
        "  dataset = tf.data.TFRecordDataset(\n",
        "      tf.io.gfile.glob(tf_records),\n",
        "      buffer_size=16 * 1024 * 1024,\n",
        "      num_parallel_reads=n_cpu)          \n",
        "  dataset = dataset.map(parse_function, num_parallel_calls=n_cpu)  \n",
        "  dataset = dataset.batch(Data_size, drop_remainder=True)  \n",
        "  dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKx3ENWdwlh8",
        "colab_type": "text"
      },
      "source": [
        "#Load the model and Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3hixJb9dG4_K",
        "colab": {}
      },
      "source": [
        "download_from_gcs(PRETRAINED_KERAS_MODEL, 'pretrained_model.h5')\n",
        "with strategy.scope():\n",
        "  model = tf.keras.models.load_model('pretrained_model.h5', compile=False)\n",
        "  model.compile(\n",
        "      optimizer=tf.train.AdamOptimizer(), loss=weighted_binary_crossentropy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4p7629BQ6mPP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "b7922347-cb53-4254-ec1b-6bfaba34fe53"
      },
      "source": [
        "#All pictures\n",
        "example_predictions = model.predict(get_dataset(valid=True)) #, steps=3\n",
        "print('shape: {}, min: {}, max: {}'.format(example_predictions.shape,\n",
        "                                           example_predictions.min(),\n",
        "                                           example_predictions.max()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Entity <function parse_function at 0x7f4bc1cd6730> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <function parse_function at 0x7f4bc1cd6730> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <function _make_execution_function_without_cloning.<locals>.distributed_function at 0x7f4bc1bbeb70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <function _make_execution_function_without_cloning.<locals>.distributed_function at 0x7f4bc1bbeb70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "shape: (1337, 14), min: -18.4959716796875, max: 3.1816318035125732\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhPleHq_nJbu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "80bceed9-1ebf-4510-969f-799117d41bf7"
      },
      "source": [
        "N_VALID = example_predictions.shape[0]\n",
        "\n",
        "dataset=extract_dataset_label(valid=True)\n",
        "for x,y in dataset:\n",
        "  groundtruth=y\n",
        "label3D=np.round(groundtruth.numpy())\n",
        "label2D=label3D[:,0,:]\n",
        "\n",
        "dataset_name = extract_dataset_name (valid=True)\n",
        "for p,s,i in dataset_name:\n",
        "  patient=p\n",
        "  study=s\n",
        "  image=i\n",
        "dataset_label=extract_original_label(valid=True)\n",
        "for p in dataset_label:\n",
        "  original_label=p"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Entity <function parse_image_name at 0x7f4c3f71dea0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <function parse_image_name at 0x7f4c3f71dea0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <function parse_original_label at 0x7f4c3f71d488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <function parse_original_label at 0x7f4c3f71d488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0e-pac96YjS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "true_all = groundtruth[:,0,:].numpy().copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8G6MDS6ExJ5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "655ebfd7-9a21-491a-8688-71d9944b006f"
      },
      "source": [
        "import statistics\n",
        "# use min((1-tpr)^2+ (fpr) ^2 )\n",
        "ss=0\n",
        "\n",
        "bootstrapped_auc_all= []\n",
        "\n",
        "for label in range(14): \n",
        "\n",
        "  true = groundtruth[:,0,label].numpy()\n",
        "  predicted = example_predictions[:,label]\n",
        "\n",
        "  if(no_remove):\n",
        "    true_tmp=true\n",
        "    predicted_tmp=predicted\n",
        "  else:\n",
        "    true_tmp=true[~np.in1d(range(len(true)),remove_list)]\n",
        "    predicted_tmp=predicted[~np.in1d(range(len(predicted)),remove_list)]\n",
        "\n",
        "\n",
        "  #precision, recall, thresholds = sklearn.metrics.precision_recall_curve(true, predicted)\n",
        "  #average_precision = sklearn.metrics.average_precision_score(true, predicted)\n",
        "  \n",
        "  #fpr, tpr, thresholds = sklearn.metrics.roc_curve(true, predicted)\n",
        "  if(len(np.where(true_tmp==1)[0])>1):\n",
        "    \n",
        "    \n",
        "    # precision, recall, thresholds = sklearn.metrics.precision_recall_curve(true_tmp, predicted_tmp)\n",
        "    # average_precision = sklearn.metrics.average_precision_score(true_tmp, predicted_tmp)\n",
        "\n",
        "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(true_tmp, predicted_tmp)\n",
        "    optimal_idx = np.argmin((1-tpr)**2+ (fpr)**2 )\n",
        "    optimal_threshold = thresholds[optimal_idx]\n",
        "    predicted_class = np.zeros(len(predicted_tmp))\n",
        "    predicted_class[predicted_tmp > optimal_threshold] = 1\n",
        "    tn, fp, fn, tp = sklearn.metrics.confusion_matrix(true_tmp, predicted_class).ravel()\n",
        "\n",
        "    roc_auc_origin = sklearn.metrics.roc_auc_score(true_tmp, predicted_tmp)\n",
        "    specificity_origin = tn / (tn + fp)\n",
        "    sensitivity_origin = tp / (tp + fn) # recall\n",
        "    PPV_origin = tp / (tp + fp) #precision\n",
        "    NPV_origin = tn / (tn + fn) #NPV\n",
        "    f1_origin = 2 * ((tp / (tp + fp)) * (tp / (tp + fn)) )/ ((tp / (tp + fp)) + (tp / (tp + fn)))\n",
        "    acc_origin = (tp + tn) / (tn + fp + fn + tp)\n",
        "\n",
        "    n_bootstraps = 1000\n",
        "    rng_seed = 47  # control reproducibility\n",
        "    bootstrapped_auc = []\n",
        "    bootstrapped_specificity = []\n",
        "    bootstrapped_sensitivity = []\n",
        "    bootstrapped_PPV = []\n",
        "    bootstrapped_NPV = []\n",
        "    bootstrapped_f1 = []\n",
        "    bootstrapped_acc = []\n",
        "\n",
        "    rng = np.random.RandomState(rng_seed)\n",
        "    for i in range(n_bootstraps):\n",
        "        # bootstrap by sampling with replacement on the prediction indices\n",
        "        indices = rng.randint(0, len(predicted_tmp), len(predicted_tmp))\n",
        "        while len(np.unique(true_tmp[indices])) < 2:\n",
        "            # We need at least one positive and one negative sample for ROC AUC\n",
        "            # to be defined: reject the sample\n",
        "            indices = rng.randint(0, len(predicted_tmp), len(predicted_tmp))\n",
        "\n",
        "\n",
        "        fpr, tpr, thresholds = sklearn.metrics.roc_curve(true_tmp[indices], predicted_tmp[indices])\n",
        "        optimal_idx = np.argmin((1-tpr)**2+ (fpr)**2 )\n",
        "        optimal_threshold = thresholds[optimal_idx]\n",
        "        predicted_class = np.zeros(len(predicted_tmp))\n",
        "        predicted_class[predicted_tmp > optimal_threshold] = 1\n",
        "        tn, fp, fn, tp = sklearn.metrics.confusion_matrix(true_tmp[indices], predicted_class[indices]).ravel()\n",
        "\n",
        "        auc = sklearn.metrics.roc_auc_score(true_tmp[indices], predicted_tmp[indices])\n",
        "        specificity = tn / (tn + fp)\n",
        "        sensitivity = tp / (tp + fn) # recall\n",
        "        PPV = tp / (tp + fp) #precision\n",
        "        NPV = tn / (tn + fn) #NPV\n",
        "        # f1 = 2 * ((tp / (tp + fp)) * (tp / (tp + fn)) )/ ((tp / (tp + fp)) + (tp / (tp + fn)))\n",
        "        f1 = sklearn.metrics.f1_score(true_tmp[indices], predicted_class[indices])\n",
        "        acc = (tp + tn) / (tn + fp + fn + tp)\n",
        "\n",
        "        bootstrapped_auc.append(auc)\n",
        "        bootstrapped_specificity.append(specificity)\n",
        "        bootstrapped_sensitivity.append(sensitivity)\n",
        "        bootstrapped_PPV.append(PPV)\n",
        "        bootstrapped_NPV.append(NPV)\n",
        "        bootstrapped_f1.append(f1)\n",
        "        bootstrapped_acc.append(acc)\n",
        "\n",
        "\n",
        "    bootstrapped_auc_all.append(bootstrapped_auc)\n",
        "\n",
        "        #print(\"Bootstrap #{} ROC area: {:0.3f}\".format(i + 1, score))\n",
        "\n",
        "    # print('[' + Labels(label).name + ']')\n",
        "    print(\"{:0.4f} {:0.4f}\".format(statistics.mean(bootstrapped_auc), statistics.stdev(bootstrapped_auc)))      \n",
        "    print(\"{:0.2f} {:0.2f}\".format(statistics.mean(bootstrapped_sensitivity)*100, statistics.stdev(bootstrapped_sensitivity)*100))\n",
        "    print(\"{:0.2f} {:0.2f}\".format(statistics.mean(bootstrapped_specificity)*100, statistics.stdev(bootstrapped_specificity)*100)) \n",
        "    # print(\"{:0.2f} {:0.2f}\".format(statistics.mean(bootstrapped_PPV)*100, statistics.stdev(bootstrapped_PPV)*100))\n",
        "    # print(\"{:0.2f} {:0.2f}\".format(statistics.mean(bootstrapped_NPV)*100, statistics.stdev(bootstrapped_NPV)*100))\n",
        "    print(\"{:0.4f} {:0.4f}\".format(statistics.mean(bootstrapped_f1), statistics.stdev(bootstrapped_f1)))\n",
        "    print(\"{:0.2f} {:0.2f}\".format(statistics.mean(bootstrapped_acc)*100, statistics.stdev(bootstrapped_acc)*100))\n",
        "\n",
        "    # print('[' + Labels(label).name + ']')\n",
        "    # print(f'AUC: {roc_auc:.2f}')\n",
        "    # print(f'Sensitivity: {sensitivity:.2f}')\n",
        "    # print(f'Specificity: {specificity:.2f}')\n",
        "    # print(f'PPV: {PPV:.2f}')\n",
        "    # print(f'NPV: {NPV:.2f}')    \n",
        "    # print(f'F1-score: {f1:.2f}')\n",
        "    # print(f'Accuracy: {acc:.2f}')\n",
        "\n",
        "    ss = len(np.where(true_tmp==1)[0]) + ss\n",
        "    #print(average_precision)\n",
        "  else:\n",
        "    print('No cases')\n",
        "  #print('N=' + str(len(np.where(true_tmp==1)[0])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7988 0.0154\n",
            "76.30 3.12\n",
            "72.09 2.28\n",
            "0.4674 0.0239\n",
            "72.75 1.80\n",
            "0.6647 0.0321\n",
            "65.11 6.66\n",
            "60.71 6.24\n",
            "0.1474 0.0202\n",
            "60.94 5.69\n",
            "0.7343 0.0252\n",
            "67.35 6.08\n",
            "69.75 6.71\n",
            "0.3060 0.0324\n",
            "69.52 5.59\n",
            "0.6919 0.0144\n",
            "67.09 2.51\n",
            "62.15 2.27\n",
            "0.6112 0.0163\n",
            "64.23 1.28\n",
            "0.6177 0.0436\n",
            "55.96 6.23\n",
            "65.81 6.76\n",
            "0.1194 0.0231\n",
            "65.40 6.39\n",
            "0.7355 0.0158\n",
            "68.46 3.68\n",
            "67.17 3.48\n",
            "0.4986 0.0208\n",
            "67.47 2.16\n",
            "0.7229 0.0334\n",
            "63.40 5.90\n",
            "67.62 6.07\n",
            "0.1421 0.0256\n",
            "67.45 5.72\n",
            "0.6109 0.0603\n",
            "58.51 9.12\n",
            "59.48 6.56\n",
            "0.0502 0.0132\n",
            "59.47 6.41\n",
            "0.6125 0.0195\n",
            "58.40 5.14\n",
            "58.68 5.17\n",
            "0.3176 0.0199\n",
            "58.63 3.70\n",
            "0.6347 0.0388\n",
            "58.18 5.76\n",
            "65.22 4.49\n",
            "0.1467 0.0216\n",
            "64.86 4.18\n",
            "0.8251 0.0122\n",
            "76.35 2.72\n",
            "74.05 2.62\n",
            "0.6327 0.0184\n",
            "74.71 1.56\n",
            "0.6265 0.0958\n",
            "44.84 15.35\n",
            "65.09 14.59\n",
            "0.0193 0.0110\n",
            "64.96 14.46\n",
            "0.5637 0.0387\n",
            "62.06 6.70\n",
            "54.50 4.51\n",
            "0.1016 0.0161\n",
            "54.81 4.22\n",
            "0.6631 0.0147\n",
            "60.84 4.02\n",
            "63.24 4.05\n",
            "0.6053 0.0200\n",
            "62.09 1.30\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}